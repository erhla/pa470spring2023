---
title: "Detroit"
date: 2023-02-09
categories: ["R"]
tags: ["detroit"]
publishdate: 2023-02-02
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      include = FALSE)
```

## Assignment

Instead of traditional problem sets, this course has a single four part assignment where you will build upon your previous work each week with new material from the course. You will explore property assessment in Detroit, Michigan and create an assessment model. After the completion of the assignment, you will wrap your model into a report which analyzes the effectiveness of your model based on the ethical and other frameworks from class and make a brief presentation to the class.

## Submissions

You will do all of your work for this assignment by creating a repository in [GitHub Classroom](https://classroom.github.com/a/m8Yk_ZMs). Please submit your work via descriptively worded commits such as 'part 1 final submission.' I will provide feedback via pull requests so please do not merge/close the pull request. Please add your GitHub username to [this sheet](https://docs.google.com/spreadsheets/d/1XwYqZwJW8L7tJHesI4juo8xlFeXWVi0Kw7B6L450om8/edit?usp=sharing) to facilitate grading.

## Part 1 (Due 2/13, 11:59pn)

You have been tasked with undertaking a multi-part analysis of homes in Detroit, Michigan. You are provided with a database to facilitate this analysis. This database was constructed from the [Detroit Open Data portal](https://data.detroitmi.gov) and numerous FOIA requests. More information is included in the databases *readme*. Note that the database must be downloaded via the link provided in the readme.

Using `part_1.Rmd` in the reports folder, edit that file to accomplishes the following tasks:

- Section A: Conduct an exploratory data analysis of homes in Detroit. Offer an overview of relevant trends in the data and data quality issues. Contextualize your analysis with [key](https://scholarship.law.uci.edu/ucilr/vol9/iss4/3/) [literature](https://harris.uchicago.edu/files/evalrespropertytaxasdetroit20162018.pdf) on properties in Detroit.
- Section B: Use [`cmfproperty`](https://erhla.github.io/cmfproperty/index.html) to conduct a sales ratio study across the relevant time period. Note that `cmfproperty` is designed to produce Rmarkdown reports but [use the documentation](https://erhla.github.io/cmfproperty/articles/cmfproperty.html) and insert relevant graphs/figures into your report. Look to make this reproducible since you'll need these methods to analyze your assessment model later on. Detroit has many sales which are not arm's length (sold at fair market value) so some sales should be excluded, but which ones?
- Section C: Explore trends and relationships with property sales using simple regressions
- Section D: Explore trends and relationships with foreclosures using simple regressions

## [Part 1 Example](/solutions/part_1.Rmd)
## [Part 1 Example HTML](/solutions/part_1.html)


## Part 2 (Due 2/27, 11:59pm)

Objective: Now that you have a decent understanding of the landscape in Detroit, create a new file (part_2.Rmd) which builds upon part_1 in the html report Rmarkdown style.

Submission: submit to Blackboard both your code and the knitted Rmarkdown output.

Part A

Create an 'introduction' to your report. Generally, only include stylized output (do not use base R print). This could mean using stargazer to show regressions, DT::datatable to show data.frames, and adding titles/labels to plots. Your introduction should include:

1. Brief background (2-3 sentences) on issues in the Detroit assessment space
2. 3 to 4 graphs with descriptive captions which include information on sale price, assessment accuracy, foreclosures, and outliers. Generally focus on single family homes and arm's length transactions. While it is notable that so many properties are sold for small amounts, we typically only want to look at properties which are class 401, taxable (e.g. assessed over 2000 or so), and sell above $4,000.

Part B

We have two separate (but very related) problems we want to model. First, we want to find a way to identify if a home is likely to be overassessed in a given year. We will analyze homes and assessments from 2016. We will use tidymodels to create a workflow.

1. Create your workflow
2. Add to your workflow a classification model
3. Add to your workflow a recipe of preprocessing steps. Use 2016 sales and assessments with the parcels property characteristics (note that we only know if a home was overassessed if it sold). Create a classification metric of overassessment based on properties which sold and use this as your **dependent** variable. Explain how you decided to construct this metric and how many classes it has.
4. Create testing/training data and evaluate your model using the classification metrics from tables 8.3 and 8.4 from the textbook and the classification probability metric ROC curves.

## [Part 2 Example](/solutions/part_2.Rmd)
## [Part 2 Example HTML](/solutions/part_2.html)

Part C

Second, building off of the workflow from part B. Create a second model to create your own 2019 assessments. (Note that I am choosing this year to avoid impacts from the pandemic and data quality issues. You may, if you'd like, create 2022 assessments. Limited sales data is released [here](https://detroitmi.gov/departments/office-chief-financial-officer/ocfo-divisions/office-assessor/sales-study).)

1. Create your workflow
2. Add to your workflow a model
3. Add to your workflow a recipe of preprocessing steps. Use sales and assessments from before 2019 with the parcels property characteristics.
4. Create testing/training data and evaluate your model using numeric metrics RMSE and MAPE.

## Part 3 (Due 3/20, 11:59pm)

Create `part_3.Rmd` in the reports folder, copying the yaml/framework from `part_1.Rmd`.

**NOTE: I have updated [the database](https://uic365-my.sharepoint.com/:u:/g/personal/erhlango_uic_edu/EaeVtM_p_eVEjzAHixZ5YrQBAUpI2nPtvgXcpYoJr1ZT3Q) with a new table, `attributes`, which includes better property characteristic information.** 

Rough codebook:

```{}
exterior, (1 siding, 2 brick/other, 3 brick, 4 other)
bath, (1 1.0, 2 1.5, 3 2 to 3, 4 3+)
height, (1 1 to 1.5, 2 1.5 to 2.5, 3 3+)
```

### Part A (20%)

Begin to finalize your report by only including report/website quality output. By this, only include stylized output (do not use base R print). Avoid any package loading or other incidential inclusion of output in your report. This could mean using stargazer to show regressions, DT::datatable to show data.frames, and adding titles/labels to plots. Give all plots and tables appropriate captions. Write two to three sentence introductions for the different sections of your report. Your report should introduce property assessment in Detroit, Michigan (as defined in part 2A), your two prediction models (from 2B and 2C), and a conclusion. 

### Part B (20%)

Feature engineering. You have now created two base models and evaluation metrics from last week. Investigate creating at least two new predictors and analyze if they improve your model. Some possibilities:

- Neighborhood foreclosures/blight tickets
- Previous rates of assessment (part B only)
- Census variables (such as income, race)
- Neighborhood demolition of homes (look at how many class 401 properties w/ nonzero assessment by year) 
- Sale price per square foot for neighborhood

### Part C (20%)

Prediction. Create out of sample predictions for both models. By this, predict overassessment and assessment/valuation for homes which did not sell for each model (2016 for B, 2019 for C). Note if you are having trouble with this step that you cannot use any information specific to the sale of a property for out-of-sample prediction.

### Part D (40%)

Model Explanation. Each model has different tools for explainability and we will discuss this more in class. Undertake this initial work knowing that we will gain more techniques for this later on.

For Model B/overassessment, aggregate your predictions by census tract. Join in a census variable. Create a simple correlation plot and create a representation of the geographic variance in your predictions (this could be a leaflet map by census tract for example).

For Model C/assessment, undertake an initial analysis of which factors your model identified as most important for valuation. 

## [Part 3 Example](/solutions/part_3.Rmd)
## [Part 3 Example HTML](/solutions/part_3.html)

## Final Submission (Due 3/31)

Create `final_report.Rmd` in the reports folder, copying the yaml/framework from `part_1.Rmd`.

Bring together your previous submissions into one cohesive report. This report should offer a brief overview of the problem (assessment), general trends on properties, your model, why your model is better than other models, and any technical or ethical critiques. 

Your final submission will build upon your part 3 submission by 'switching out' the model you use and adding a conclusion.

### Part A, New Assessment Models

Mirror Section 15.3 from the textbook for your assessment model only!

- Create a `workflow_set()` of three different model types. You may choose any which are comparable with `tidymodels`. Suggested models and tuning parameters below. I **encourage** you to consider using one model not from this list, but that is optional.

```
linear_reg_spec <- 
   linear_reg(penalty = tune(), mixture = tune()) %>% 
   set_engine("glmnet")
   
rf_spec <- 
   rand_forest(mtry = tune(), min_n = tune(), trees = 250) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")
   
xgb_spec <- 
   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), 
              min_n = tune(), sample_size = tune(), trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")
   
my_set <- workflow_set(
  preproc = list(name_for_your_recipie = your_recipie),
  models = list(linear_reg = linear_reg_spec, random_forest = rf_spec, boosted = xgb_spec)
)
```

The textbook applies slightly different preprocessing steps to these models, but they should work reasonably well with your current recipe. If you have any compatability issues, I recommend starting with a simple recipe and adding things back one at a time.

- Now, apply `workflow_map()` to your `workflow_set()`. You should resample your testing data and create a small grid. Please use at least 3 resamples and a grid of at least 5. This may take a long time (10+ minutes). I recommend starting very small and gradually increasing your resamples/grid. Note that I've included verbose = TRUE here to help you debug, please do not include this output in your final project.

```
grid_ctrl <-
   control_grid(
      save_pred = FALSE,
      save_workflow = FALSE
   )

grid_results <-
   my_set %>%
   workflow_map(
      seed = 1503,
      resamples = your_resamples,
      grid = 5,
      control = grid_ctrl,
      verbose = TRUE
   )
   
```

- Now, use `rank_results` on your selected performance metric and `autoplot` to replicate figure 15.1. Does one model perform significantly better than others? Select what you feel is the best by finalizing your model (see Section 15.5).

```
best_results <- 
   grid_results %>% 
   extract_workflow_set_result("best model type name") %>% 
   select_best(metric = "your metric")
best_results

best_results_fit <- 
   grid_results %>% 
   extract_workflow("best model type name") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = your_rsample_data) #this is the output of rsample::initial_time_split() or rsample::initial_split()
```

- Consider making a simple visualization of predicted / observed values from your best model similar to Figure 15.5

```
best_results_fit %>% 
   collect_predictions() %>% 
   ggplot(aes(x = target_variable, y = .pred)) + 
   geom_abline(color = "gray50", lty = 2) + 
   geom_point(alpha = 0.5) + 
   coord_obs_pred() + 
   labs(x = "observed", y = "predicted")
```

### Part B, Hyperparameter Exploration for Classification

Mirroring Section 14.2.3, take your current workflow and use `tune_bayes()` to create a small tuning grid for your classification model. You will need to:

- Identify appropriate hyperparameters to be tuned for your chosen model type and set them equal to `tune()` in your workflow (note: do not include `mtry` in your tuning grid)
- Manually create a start_grid and evaluate your workflow to create initial values for `tune_bayes()`

```
initial_vals <- your_workflow %>%
  tune_grid(
    resampled_data,
    grid = 4,
    metrics = your_metric_set,
  )
```

- Run a bayes search

```
ctrl <- control_bayes(verbose = TRUE)

your_search <- 
  your_workflow %>%
  tune_bayes(
    resamples = ...,
    metrics = ...,
    initial = initial_vals, #note you may simply pass a number here e.g. 6 for a random search
    iter = 25,
    control = ctrl
  )
```
- Call `show_best` and finalize your model

### Part C, Conclusion & Presentation

Write a four paragraph conclusion to your file. Include information on your model type, its performance on your chosen objective function, any ethical or implementation issues (e.g. should Detroit use your model?).

In class on the 31st, everyone will give a brief presentation on their work. You may present your knitted Rmd file or pull some of your graphs into a slide deck. Your presentation should be at most five minutes. Broadly look to answer if your model should be implemented by discussing the information in your conclusion and assignment.


## Grading Overview

For each assignment, you will be graded on substantial completion of the assignment (demonstrated by an attempt of all parts). When submitting parts 2, 3, and 4, you will be additionally graded on your incorporation of feedback, new concepts from the course, or the correction of any flagged issues.

The assignment will culminate in a final submission of code/report and presentation. Code will be graded based on reproducibility, conceptual understanding, and accuracy. The report will be an Rmarkdown file which knits together graphs, tables, and ethical frameworks. It should be concise (include only relevant information from Parts 1-4). This report will be used to give a five minute presentation to the class on your model and ethical/technical issues with Detroit property assessment.

## [Part 4 Example](/solutions/part_4.Rmd)
## [Part 4 Example HTML](/solutions/part_4.html)


| Date   | Points | Category                                     | Notes                                                                           |
|-----------|-----------|----------------------------------------------|-------------------------------------------|
| 10-Feb | 5      | Substantial Completion (attempted all parts) | Part 1 Due                                                                      |
| 24-Feb | 5      | Substantial Completion (attempted all parts) | Part 2 Due                                                                      |
| 24-Feb | 5      | Incorporation of Feedback/New Concepts       | From Part 1                                                                     |                                                         |
| 17-Mar | 10      | Substantial Completion (attempted all parts) | Part 3 Due                                                                      |
| 17-Mar | 10      | Incorporation of Feedback/New Concepts       | From Part 2                                                                     |
| 31-Mar | 30     | Final Code                                   | Reproducible (10), Concepts (10), Accurate (10)                                 |
| 31-Mar | 20     | Final Report                                 | Via Rmarkdown HTML, contextualized analysis and ethics |
| 31-Mar | 15     | Final Presentation                           | 5 minute presentation on model and insights                                     |
