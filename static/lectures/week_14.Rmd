---
title: "Week 14"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    theme: sandstone
    number_sections: true
    code_folding: hide
---

```{r initial_setup, include=FALSE}

knitr::opts_chunk$set(warning = FALSE)

library(tidymodels)
library(tidyverse)
library(tidytext)
library(textrecipes)
library(tokenizers)
library(janeaustenr)
library(stopwords)
library(SnowballC)
library(widyr)
library(quanteda)
#devtools::install_github("EmilHvitfeldt/scotus")

```




# Natural Language Processing (NLP) / Supervised Modeling for Text

This lecture is based on [Supervised Machine Learning for Text Analysis in R](https://smltar.com/preface.html).

Similar to our previous work we can have both regression and classification text based problems. A regression model would predict a numeric/continuous output 'such as predicting the year of a United States Supreme Court opinion from the text of that opinion.' A classification model would predict a discrete class 'such as predicting whether a GitHub issue is about documentation or not from the text of the issue.'

Natural language needs to be standardized and transformed to numeric representations for modeling. We will use the `textrecipes` package to do this. 

# Preprocessing

What language is and how language works is key to creating modeling features from natural language. Words in English are made of prefixes, suffixes, and root words. Defining a word can be quite difficult with compound words (like real estate or dining room). Preprocessing natural language has three primary steps: tokenization, removal of stop words, and even stemming.

## Tokenization

Tokenization can broadly be thought of as taking an input (such as a string) and a token type (such as a word) and splitting the input into pieces/tokens. This process is generally much more complex than you might think (e.g. more than splitting on non-alphanumeric characters) and `spaCy` through the (`tokenizers` package) implements a fast tool set.

Tokens have a variety of units including characters, words, sentences, lines, paragraphs, and n-grams.

```{r}
sample_vector <- c("Far down in the forest",
                   "grew a pretty little fir-tree")
sample_tibble <- tibble(text = sample_vector)

tokenize_words(sample_vector)

```



```{r}
sample_tibble %>%
  unnest_tokens(word, text, token = "words")
```


```{r}
sample_tibble %>%
  unnest_tokens(word, text, token = "words", strip_punct = FALSE)
```

```{r}
pride <- tibble(line=janeaustenr::prideprejudice)

pride %>%
  unnest_tokens(word, line) %>%
  count(word) %>%
  arrange(desc(n))
```

### n-grams

n-gram is a contiguous sequence of n items from a given sequence of text. Examples:

- unigram: “Hello,” “day,” “my,” “little”
- bigram: “fir tree,” “fresh air,” “to be,” “Robin Hood”
- trigram: “You and I,” “please let go,” “no time like,” “the little mermaid”

```{r}
token_ngram <- tokenize_ngrams(x = pride %>% pull(line),
                                   lowercase = TRUE,
                                   n = 3L,
                                   n_min = 3L,
                                   stopwords = character(),
                                   ngram_delim = " ",
                                   simplify = FALSE)

token_ngram[[100]]
```

### Chinese

```{r}
library(jiebaR)
words <- c("下面是不分行输出的结果", "下面是不输出的结果")

engine1 <- worker(bylines = TRUE)

segment(words, engine1)
```


## Stop Words

Some words carry less information than others. For example, a, the, or of. These common words are called stop words and are generally removed entirely. Let's use the `stopwords` package here to provide some lists.

```{r}
pride_words <- 
  pride %>%
  unnest_tokens(word, line)

pride_words %>%
  semi_join(get_stopwords(source = "snowball")) %>%
  distinct() # present stop words

pride_words %>%
  anti_join(get_stopwords(source = "snowball")) %>%
  distinct() # unique non stop words
```

## Stemming

What if we aren't interested in the difference between banana and bananas? The core sentiment of a word is often the same (e.g. 'banana'). 

```{r}
pride_words %>%
  anti_join(get_stopwords(source = "snowball")) %>%
  mutate(word_stem = wordStem(word))

pride_words %>%
  anti_join(get_stopwords(source = "snowball")) %>%
  mutate(word_stem = wordStem(word)) %>%
  summarize(nword = n_distinct(word),
            nstem = n_distinct(word_stem))
```

Stemming reduces the feature space of text data but may change the underlying meaning of some sentences. It may or may not improve models.

# Word Embeddings

A data structure for text data. 

```{r}
complaints <- read_csv("https://github.com/EmilHvitfeldt/smltar/raw/master/data/complaints.csv.gz")


complaints %>%
  slice_sample(n=10000) %>%
  unnest_tokens(word, consumer_complaint_narrative) %>%
  anti_join(get_stopwords(), by = "word") %>%
  mutate(stem = wordStem(word)) %>%
  count(complaint_id, stem) %>%
  cast_dfm(complaint_id, stem, n)
```


This is a sparse matrix (where most elements are zero). This is because most documents do not contain most words.

We could also represent text data with weighted counts. The term frequency of a word is how frequently a word occurs in a document, and the inverse document frequency of a word decreases the weight for commonly-used words and increases the weight for words that are not used often in a collection of documents.


$$idf(\text{term}) = \ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}$$
\index{term frequency-inverse document frequency|see {tf-idf}}
These two quantities can be combined to calculate a term's \index{tf-idf}tf-idf (the two quantities multiplied together). This statistic measures the frequency of a term adjusted for how rarely it is used, and it is an example of a weighting scheme that can often work better than counts for predictive modeling with text features. 

```{r}
complaints %>%
  slice_sample(n=10000) %>%
  unnest_tokens(word, consumer_complaint_narrative) %>%
  anti_join(get_stopwords(), by = "word") %>%
  mutate(stem = wordStem(word)) %>%
  count(complaint_id, stem) %>%
  bind_tf_idf(stem, complaint_id, n) %>%
  cast_dfm(complaint_id, stem, tf_idf)
```

Creating these matrices is very memory intensive!

While you can create your own embeddings, pre-trained word embeddings such as GloVe which is training on Wikipedia and news sources are readily available.

```{r}
library(textdata)

glove6b <- embedding_glove6b(dimensions = 100)
glove6b

```

```{r}
tidy_glove <- glove6b %>%
  pivot_longer(contains("d"),
               names_to = "dimension") %>%
  rename(item1 = token)

rm(glove6b)

tidy_glove
```





```{r}
nearest_neighbors <- function(df, token) {
  df %>%
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) / 
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
        matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE,
      maximum_size = NULL
    )(item1, dimension, value) %>%
    select(-item2)
}


```

Let's look at words which are 'synonyms' or nearby in the embedding space...

```{r}
tidy_glove %>%
  nearest_neighbors("error")


tidy_glove %>%
  nearest_neighbors("school")


tidy_glove %>%
  nearest_neighbors("fee")


tidy_glove %>%
  nearest_neighbors("chocolate")
```


## Fairness/ethics

Do word embeddings create systemic unfairness or bias? Yes! Here's some examples from GloVe:

- Typically Black first names are associated with more unpleasant feelings than typically white first names.

- Women’s first names are more associated with family and men’s first names are more associated with career.

- Terms associated with women are more associated with the arts and terms associated with men are more associated with science.


