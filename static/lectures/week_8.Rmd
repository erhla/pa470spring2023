---
title: "Week 8"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    theme: sandstone
    number_sections: true
---

```{r initial_setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(lubridate)
library(kableExtra)
library(scales)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

theme_set(theme_bw())
```

# Introduction

Now that we have begun to see different models and concepts, hopefully all the pieces are starting to come together. Prediction problems are hard and there's no single way which works for every solution but generally here steps which will be applicable for most of our work.

## Identify Prediction Task

Since we are only focused on supervised learning here, to identify the task we need adequate labeled data and an idea of if our outcome is a class or a continous variable.

## Prepare Data

This task varies depending on the requirements of our model. Some models may only be able to ingest certain types of data (one hot encoded matrices for example). Other variables may need to be rescaled or imputed. We can codify all of our preprocessing steps into a recipe.

## Construct Formula

There are a variety of tests to determine which variables to include in the formula. This step is very arbitrary and generally EDA is the answer.

## Select Model

There are hundreds of model types. In this class, we are going to use  boosted trees, decision trees, linear regression, logistic regression, nearest neighbors, and random forests from parnsip.

## Build Pipeline

Your pipeline is where you bring the steps above into a cohesive object and is the foundation of our process. A pipeline consists of a workflow, recipe, and model. In typical ML applications, hundreds or thousands of models will be built on this foundation in order to find the best model specification.

## Now...Creating Effective Models

Ch 10-15 of tidymodels offer methods to create numerous models and select the best for predictive performance. In combination, these methods will artificially create hundreds of different datasets by resampling, and try hundreds of different hyperparameter specifications. We will then select the 'best' model based on an additional objective functions (such as accuracy or RMSE).


# Resampling

Section 10.1 in tidymodels give an example of a linear model and a random forest model which perform differently on training and testing to frame why resampling is important.

|          |    RMSE Estimates   |          |
|:--------:|:-------------------:|:--------:|
|  object  |              train  |    test  |
|  lm_fit  |             0.0754  |  0.0736  |
|  rf_fit  |             0.0365  |  0.0704  |

In this scenario, the random forest model performed much better on training than testing since it overfit the training data. If we were going to pick which model to use based only on training RMSE performance, this could have led to some issues!

## Overview

Resampling involves splitting, slicing, and bootstrapping the training set. In tidymodels, this could involve creating ten sets of analysis and assessment subsamples from the training data. Then model performance would then be generated by averaging the performance across these samples.

Recall our example from week 6,

```{r}
data(cells, package = "modeldata")

cell_split <- initial_split(cells %>% select(-case), 
                            strata = class)
cell_train <- training(cell_split)
folds <- rsample::vfold_cv(cell_train, v = 10)

cell_test  <- testing(cell_split)


rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

rf_wf <- 
  workflow() %>%
  add_model(rf_mod) %>%
  add_formula(class ~ .)

```

Our folds, using 5-fold cross validation:

```{r}
folds <- rsample::vfold_cv(cell_train, v = 5)
folds
```

Fold 1:

```{r}
folds[[1]][[1]]
```

We then fit the model on each resample.


```{r}
rf_fit_rs <- 
  rf_wf %>% 
  fit_resamples(folds, control=control_resamples(save_pred=TRUE))
```

You can see the model performance on each fold...

```{r}
collect_metrics(rf_fit_rs, summarize=FALSE)
```

Or aggregated.

```{r}
collect_metrics(rf_fit_rs, summarize=TRUE)
```

We can also aggregate the folds to get predictions from our training data.

```{r}
rf_testing_pred <- collect_predictions(rf_fit_rs, summarize = TRUE)


rf_testing_pred %>%                   
  accuracy(truth = class, .pred_class)

rf_testing_pred %>%                   
  roc_curve(truth = class, .pred_PS) %>%
  autoplot()
```

## Variations

### Repeated CV

In the thread of using resampling to get more data on our model performance, a key variation is repeated cross-validation.

`vfold_cv(data, v=10, repeats=5)`

### Bootstrapping

Bootstrapping creates samples drawn with replacement.

`bootstraps(data, times=5)`

This code would create 50 folds.

## Summary

During resampling, the analysis set is used to preprocess the data, apply the preprocessing to itself, and use these processed data to fit the model.

The preprocessing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance.

# Models

## Note on Random Forest Explainability

Partial dependence: A way of determining which variables are the most important to the model. This is not the same as a model coefficient but roughly a way of seeing what the model relies on. This can be useful to understand your model and explain it to others. For example, from Table 10.8 in DSPP, numbers of hours worked provides the most information in that random forest model to predict wages. If instead, the most important variable was something unexpected you would expect to be completed random like the weather, you might want to respecify your model.

There are our methods to decompose effects mentioned in the textbook such as Locally Interpretable Model-Agnostic Explanations (LIME). If we have time at the end of the course, we will revist these.

## Gradient Boosting

The CCAO presented talked about gradient boosting (lightGBM). LightGBM is not as well integrated into tidymodels as the other boosted trees.

Gradient boosting is very similar to decision trees which gradually improves itself. I will show an example using XGBoost later on. Part of making a good model here involves cleverly creating predictor variables based on the context.

## Neural Networks

Figure 10.14.
