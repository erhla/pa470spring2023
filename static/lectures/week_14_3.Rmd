---
title: "Week 14"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    theme: sandstone
---

```{r initial_setup, include=FALSE}

knitr::opts_chunk$set(warning = FALSE)

library(tidymodels)
library(tidyverse)
library(applicable)
library(patchwork)
library(probably)

```

# Ch 16 Dimensionality Reduction

Dimensionality reduction transforms a data set from a high-dimensional space into a low-dimensional space, and can be a good choice when you suspect there are “too many” variables. An excess of variables, usually predictors, can be a problem because it is difficult to understand or visualize data in higher dimensions.

- Principal component analysis (PCA) unsupervsied and uncorrelated

![](https://www.tmwr.org/16-dimensionality-reduction_files/figure-html/bean-pca-1.png)

![](https://www.tmwr.org/16-dimensionality-reduction_files/figure-html/pca-loadings-1.png)

- Partial least squares (PCA) supervised

"It tries to find components that simultaneously maximize the variation in the predictors while also maximizing the relationship between those components and the outcome."

- Independent Component Analysis (ICA) statistically independent

# Ch 19 Trusting Predictions

How do we avoid inappropriate predictions?


- Equivocal zones use the predicted values to alert the user that the results may be suspect.

- Applicability uses the predictors to measure the amount of extrapolation (if any) for new samples.


```{r trust-simulation}
library(tidymodels)
tidymodels_prefer()
simulate_two_classes <- 
  function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {
    # Slightly correlated predictors
    sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)
    dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)
    colnames(dat) <- c("x", "y")
    cls <- paste0("class_", 1:2)
    dat <- 
      as_tibble(dat) %>% 
      mutate(
        linear_pred = !!eqn,
        # Add some misclassification noise
        linear_pred = linear_pred + rnorm(n, sd = error),
        prob = binomial()$linkinv(linear_pred),
        class = ifelse(prob > runif(n), cls[1], cls[2]),
        class = factor(class, levels = cls)
      )
    dplyr::select(dat, x, y, class)
  }
set.seed(1901)
training_set <- simulate_two_classes(200)
testing_set  <- simulate_two_classes(50)
```

We estimate a logistic regression model using Bayesian methods (using the default Gaussian prior distributions for the parameters): 

```{r trust-bayes-glm}
two_class_mod <- 
  logistic_reg() %>% 
  set_engine("stan", seed = 1902) %>% 
  fit(class ~ . + I(x^2)+ I(y^2), data = training_set)
print(two_class_mod, digits = 3)
```

The fitted class boundary is overlaid onto the test set. The data points closest to the class boundary are the most uncertain. If their values changed slightly, their predicted class might change. One simple method for disqualifying some results is to call them "equivocal" if the values are within some range around 50% (or whatever the appropriate probability cutoff might be for a certain situation). Depending on the problem that the model is being applied to, this might indicate that another measurement should be collected or that we require more information before a trustworthy prediction is possible.

```{r glm-boundaries}
#| echo = FALSE, 
#| fig.width=6, 
#| fig.height=6, 
#| out.width="70%", 
#| fig.align="center",
#| fig.cap = "Simulated two class data set with a logistic regression fit and decision boundary.",
#| fig.alt = "Simulated two class data set with a logistic regression fit and decision boundary. The scatter plot of the two classes shows fairly correlated data. The decision boundary is a parabola in the x axis that does a good job of separating the classes."
data_grid <-
  crossing(
    x = seq(-4.5, 4.5, length = 100),
    y = seq(-4.5, 4.5, length = 100)
  )
grid_pred <- 
  predict(two_class_mod, data_grid, type = "prob") %>% 
  bind_cols(
    predict(two_class_mod, data_grid, type = "pred_int", std_error = TRUE),
    data_grid
  )
grid_pred %>% 
  mutate(`Probability of Class 1` = .pred_class_1) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_raster(aes(fill = `Probability of Class 1`)) +
  geom_point(data = testing_set, aes(shape = class, color = class), alpha = .75, size = 2.5) + 
  geom_contour(aes(z = .pred_class_1), breaks = .5, color = "black", lty = 2) + 
  coord_equal() + 
  labs(x = "Predictor x", y = "Predictor y", color = NULL, shape = NULL) + 
  scale_fill_gradient2(low = "#FDB863", mid = "white", high = "#B2ABD2", midpoint = .5) + 
  scale_color_manual(values = c("#2D004B", "darkorange"))
```

We could base the width of the band around the cutoff on how performance improves when the uncertain results are removed. However, we should also estimate the reportable rate (the expected proportion of usable results). For example, it would not be useful in real-world situations to have perfect performance but only release predictions on 2% of the samples passed to the model. 

Let's use the test set to determine the balance between improving performance and having enough reportable results. The predictions are created using:  

```{r trust-bayes-glm-pred}
test_pred <- augment(two_class_mod, testing_set)
test_pred %>% head()
```

With tidymodels, the `probably` package contains functions for equivocal zones. For cases with two classes, the `make_two_class_pred()` function creates a factor-like column that has the predicted classes with an equivocal zone: 

```{r trust-make-eq}
library(probably)
lvls <- levels(training_set$class)
test_pred <- 
  test_pred %>% 
  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))
test_pred %>% count(.pred_with_eqz)
```

Rows that are within $0.50\pm0.15$ are given a value of `[EQ]`. 

:::rmdnote
It is important to realize that `[EQ]` in this example is not a factor level, but an attribute of that column. 
:::

Since the factor levels are the same as the original data, confusion matrices and other statistics can be computed without error. When using standard functions from the `yardstick` package, the equivocal results are converted to `NA` and are not used in the calculations that use the hard class predictions. Notice the differences in these confusion matrices:

```{r trust-conf-mat}
# All data
test_pred %>% conf_mat(class, .pred_class)
# Reportable results only: 
test_pred %>% conf_mat(class, .pred_with_eqz)
```

There is also an `is_equivocal()` function available for filtering these rows from the data. 

Does the equivocal zone help improve accuracy? Let's look over different buffer sizes:

```{r trust-eq-calcs, eval = FALSE}
# A function to change the buffer then compute performance.
eq_zone_results <- function(buffer) {
  test_pred <- 
    test_pred %>% 
    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))
  acc <- test_pred %>% accuracy(class, .pred_with_eqz)
  rep_rate <- reportable_rate(test_pred$.pred_with_eqz)
  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)
}
# Evaluate a sequence of buffers and plot the results. 
map_dfr(seq(0, .1, length.out = 40), eq_zone_results) %>% 
  pivot_longer(c(-buffer), names_to = "statistic", values_to = "value") %>% 
  ggplot(aes(x = buffer, y = value, lty = statistic)) + 
  geom_step(size = 1.2, alpha = 0.8) + 
  labs(y = NULL, lty = NULL)
```

```{r equivocal-zone-results, ref.label = "trust-eq-calcs"}
#| echo = FALSE,
#| out.width="80%",
#| fig.cap = "The effect of equivocal zones on model performance.",
#| fig.alt = "The effect of equivocal zones on model performance. There is a slight increase in accuracy at the expense of a falling reportable rate."
```

The figure shows us that accuracy improves by a few percentage points but at the cost of nearly 10% of predictions being unusable! The value of such a compromise depends on how the model predictions will be used. 

This analysis focused on using the predicted class probability to disqualify points, since this is a fundamental measure of uncertainty in classification models. A slightly better approach would be to use the standard error of the class probability. Since we used a Bayesian model, the probability estimates we found are actually the mean of the posterior predictive distribution. In other words, the Bayesian model gives us a distribution for the class probability.  Measuring the standard deviation of this distribution gives us a _standard error of prediction_ of the probability. In most cases, this value is directly related to the mean class probability. You might recall that, for a Bernoulli random variable with probability $p$, the variance is $p(1-p)$. Because of this relationship, the standard error is largest when the probability is 50%. Instead of assigning an equivocal result using the class probability, we could instead use a cutoff on the standard error of prediction. 

One important aspect of the standard error of prediction is that it takes into account more than just the class probability. In cases where there is significant extrapolation or aberrant predictor values, the standard error might increase. The benefit of using the standard error of prediction is that it might also flag predictions that are problematic (as opposed to simply uncertain). One reason that we used the Bayesian model is that it naturally estimates the standard error of prediction; not many models can calculate this. For our test set, using `type = "pred_int"` will produce upper and lower limits and the `std_error` adds a column for that quantity. For 80% intervals: 

```{r trust-pred-int}
test_pred <- 
  test_pred %>% 
  bind_cols(
    predict(two_class_mod, testing_set, type = "pred_int", std_error = TRUE)
  )
```

For our example where the model and data are well-behaved, Figure \@ref(fig:std-errors) shows the standard error of prediction across the space: 

```{r std-errors}
#| echo = FALSE, 
#| fig.width=5, 
#| fig.height=5, 
#| out.width="70%", 
#| fig.align="center",
#| fig.cap = "The effect of the standard error of prediction overlaid with the test set data.",
#| fig.alt = "The effect of the standard error of prediction overlaid with the test set data. The region of large variation is very similar to the class boundary space. Additionally, there is a large amount of variation to the west of the inflection point of the boundary curve."
grid_pred %>% 
  mutate(`Std Error` = .std_error) %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_raster(aes(fill = `Std Error`)) + 
  scale_fill_gradientn(colours = c("#F7FBFF", "#DEEBF7", "#C6DBEF", "#9ECAE1", "#6BAED6")) + 
  geom_point(data = testing_set, aes(shape = class), alpha = .5, size = 2) + 
  coord_equal() + 
  labs(x = "Predictor x", y = "Predictor y", shape = NULL)
```

Using the standard error as a measure to preclude samples from being predicted can also be applied to models with numeric outcomes. However, as shown in the next section, this may not always work.  

